{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddea7771",
   "metadata": {},
   "source": [
    "# Baseline forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd56a64",
   "metadata": {},
   "source": [
    "## Vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4688616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLCore(pl.LightningModule):\n",
    "    \"\"\"pytorch lightning core module\"\"\"\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # stores hyperparameters in self.hparams and allows logging\n",
    "\n",
    "    def _shared_step(self, x, y):\n",
    "        \"\"\"Shared step used in training, validation and test step.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_id):\n",
    "        (x1, x2), (s1, _) = batch\n",
    "        loss = self._shared_step((x1, s1), x2)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_id):\n",
    "        (x1, x2), (s1, _) = batch\n",
    "        loss = self._shared_step((x1, s1), x2)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # log hparams with val_loss as reference\n",
    "        if self.logger:\n",
    "            val_loss = torch.min(torch.stack(outputs))\n",
    "            self.logger.log_hyperparams(self.hparams, {\"hp/epoch_val_loss\": val_loss})\n",
    "\n",
    "    def test_step(self, batch, batch_id):\n",
    "        (x1, x2), (s1, _) = batch\n",
    "        loss = self._shared_step((x1, s1), x2)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-5)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\", \"monitor\": \"train_loss\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59115bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(PLCore):\n",
    "    def __init__(self, d_in=400, d_hidden=512, d_out=150, lr=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_in, d_hidden),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(d_hidden, d_out))\n",
    "    def _shared_step(self, x, y):\n",
    "        (x1, s1) = x\n",
    "        x2 = y\n",
    "        \n",
    "        x_ = torch.cat(torch.unsqueeze())\n",
    "        \n",
    "        h1 = x[:, 0, 0]\n",
    "        h2 = x[:, 0, 1]\n",
    "        h2_all = x[:, :, 1]  # keep as time series\n",
    "        h3 = x[:, 0, 2]\n",
    "        \n",
    "        y = y[:, 0, :-1]  # q1, q3, kv1, kv2, kv3, no duration (always 50)\n",
    "        \n",
    "        y = torch.stack([h1, h2, h3, *torch.unbind(y, dim=-1)], dim=-1)\n",
    "        \n",
    "        h2_pred = self.ffn(y)\n",
    "        \n",
    "        loss = F.mse_loss(h2_all, h2_pred)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, y):\n",
    "        h2_pred = self.ffn(y)\n",
    "        return h2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c234ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1196cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
