{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddea7771",
   "metadata": {},
   "source": [
    "# Baseline forecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f05552",
   "metadata": {},
   "source": [
    "- first scenario is simple: multivariate forecasting of the system based on past behavior (both observable and control variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f83740-1a6d-4443-b056-042890c705be",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ddf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7668c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from data.data_module import ThreeTankDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebca573",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ThreeTankDataModule(batch_size=256)\n",
    "# dm.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afd56a64",
   "metadata": {},
   "source": [
    "## Vanilla LSTM\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90110909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import train_arguments as args\n",
    "\n",
    "\n",
    "\n",
    "class PLCore(pl.LightningModule):\n",
    "    \"\"\"pytorch lightning core module\n",
    "    This module is the base class for all models.\n",
    "    It implements the training, validation and test steps.\n",
    "    Args:\n",
    "        d_seq_in (int): input sequence length\n",
    "        d_features (int): number of features\n",
    "        d_seq_out (int): output sequence length\n",
    "        train_scenario (str): the scenario to train on\n",
    "        lr (float): learning rate\n",
    "        beta1 (float): beta1 for Adam optimizer\n",
    "        beta2 (float): beta2 for Adam optimizer\n",
    "        eps (float): epsilon for Adam optimizer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_seq_in=250, d_features=3, d_seq_out=50, \n",
    "                 train_scenario=\"standard\",\n",
    "                 lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_seq_in = d_seq_in\n",
    "        self.d_features = d_features\n",
    "        self.d_seq_out = d_seq_out\n",
    "        \n",
    "        self.example_input_array = torch.rand(32, self.d_seq_in, self.d_features)  # 32 as example batch size\n",
    "\n",
    "        self.save_hyperparameters()  # stores hyperparameters in self.hparams and allows logging\n",
    "        self.visualization_device = \"cpu\"  # for visualizations, can be changed in model if necessary\n",
    "\n",
    "        scenario_dict = {\n",
    "            \"standard\": 0,\n",
    "            \"fault\": 1,\n",
    "            \"noise\": 2,\n",
    "            \"duration\": 3,\n",
    "            \"scale\": 4,\n",
    "            \"switch\": 5,\n",
    "            \"q1+v3\": 6,\n",
    "            \"q1+v3+rest\": 7,\n",
    "            \"v12+v23\": 8,\n",
    "            \"standard+\": 9,\n",
    "            \"standard++\": 10,\n",
    "            \"frequency\": 11,\n",
    "            \"time_warp\": 12\n",
    "        }\n",
    "        self.train_scenario_idx = scenario_dict[train_scenario]\n",
    "        \n",
    "        # metrics to keep track of\n",
    "        metrics = {\n",
    "            \"MAE\": MeanAbsoluteError(),\n",
    "            \"MSE\": MeanSquaredError()\n",
    "        }\n",
    "        # the loss function to use for backpropagation\n",
    "        self.loss_fct_key = args.LOSS_FCT  \n",
    "        assert self.loss_fct_key in metrics.keys(), \"loss function key should be in metrics\"\n",
    "\n",
    "        self.train_metrics = nn.ModuleDict({name: metric.clone() for name, metric in metrics.items()})\n",
    "        self.val_metrics = nn.ModuleDict({name: metric.clone() for name, metric in metrics.items()})\n",
    "        self.test_metrics = nn.ModuleDict({name: metric.clone() for name, metric in metrics.items()})\n",
    "        self.validation_step_outputs = []\n",
    "        self.min_epoch_val_loss = float(\"inf\")\n",
    "\n",
    "    def _shared_step(self, x, y):\n",
    "        \"\"\"Shared step used in training, validation and test step.\n",
    "        Should return the prediction and the target (y_pred, y).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This should be implemented in the model that inherits from PLCore.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for pytorch lightning.\n",
    "        Should return the prediction (y_pred).\"\"\"\n",
    "        return self._shared_step(x, None)[0]\n",
    "    \n",
    "    def training_step(self, batch, batch_id):\n",
    "        \"\"\"Training step for pytorch lightning.\n",
    "        Only receives dataloader 0, which is the training dataloader.\n",
    "        \"\"\"\n",
    "        x1, x2 = batch\n",
    "        pred, target = self._shared_step(x1, x2)\n",
    "        for name, metric in self.train_metrics.items():\n",
    "            metric_loss = metric(pred, target)\n",
    "            self.log(\"train_\" + name, metric_loss, logger=True)\n",
    "            if name == self.loss_fct_key:\n",
    "                # use this loss function for backpropagation\n",
    "                loss = metric_loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_id, dataloader_idx):\n",
    "        \"\"\"Validation step for pytorch lightning.\"\"\"\n",
    "        x1, x2 = batch\n",
    "        pred, target = self._shared_step(x1, x2)\n",
    "        for name, metric in self.val_metrics.items():\n",
    "            metric_loss = metric(pred, target)\n",
    "            self.log(\"val_\" + name, metric_loss, logger=True)\n",
    "            if name == self.loss_fct_key and dataloader_idx == self.train_scenario_idx:\n",
    "                # save epoch losses on standard dataset for logging\n",
    "                self.validation_step_outputs.append(metric_loss)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Validation epoch end for pytorch lightning.\n",
    "        Only receives val losses from dataloader 0.\n",
    "        \"\"\"\n",
    "        epoch_losses = torch.stack(self.validation_step_outputs)\n",
    "        mean_loss = torch.mean(epoch_losses)\n",
    "        self.log(\"ep_val_loss\", mean_loss, prog_bar=True, logger=True)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        # save if this is the best model so far\n",
    "        if mean_loss < self.min_epoch_val_loss:\n",
    "            self.min_epoch_val_loss = mean_loss\n",
    "\n",
    "    def on_train_end(self):\n",
    "        \"\"\"Train end for pytorch lightning.\"\"\"\n",
    "        # log best val_loss\n",
    "        if self.logger:\n",
    "            self.logger.log_hyperparams(self.hparams, {\"hp/min_epoch_val_loss\": self.min_epoch_val_loss})\n",
    "\n",
    "    def test_step(self, batch, batch_id, dataloader_idx):\n",
    "        \"\"\"Test step for pytorch lightning.\"\"\"\n",
    "        x1, x2 = batch\n",
    "        pred, target = self._shared_step(x1, x2)\n",
    "        for name, metric in self.test_metrics.items():\n",
    "            self.log(\"test_\" + name, metric(pred, target), logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers for pytorch lightning.\"\"\"\n",
    "        optimizer = Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.lr,  \n",
    "            betas=(self.hparams.beta1, self.hparams.beta2),\n",
    "            eps=self.hparams.eps\n",
    "            )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=25, min_lr=1e-5)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\", \"monitor\": f\"val_{self.loss_fct_key}/dataloader_idx_0\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59115bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.core import PLCore\n",
    "\n",
    "\n",
    "\n",
    "class LSTM(PLCore):\n",
    "\t\"\"\"LSTM model.\n",
    "\tArgs:\n",
    "\t\td_hidden (int): hidden dimension\n",
    "\t\tn_layers (int): number of layers\n",
    "\t\tbidirectional (bool): whether to use bidirectional LSTM\n",
    "\t\tdropout (float): dropout rate\n",
    "\t\tautoregressive (bool): whether to predict one output at a time\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_hidden, n_layers=1, bidirectional=False, dropout=0.5, autoregressive=False, **kwargs):\n",
    "\t\tsuper().__init__(**kwargs)\n",
    "\t\t\n",
    "\t\t# self.d_features and self.d_seq_out from parent class\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(\n",
    "\t\t\tinput_size=self.d_features,\n",
    "\t\t\thidden_size=d_hidden,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t\tbidirectional=bidirectional,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=dropout if n_layers > 1 else 0\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tif not autoregressive:\n",
    "\t\t\t# Predict all outputs at once\n",
    "\t\t\tself.fc = nn.Linear(d_hidden * (bidirectional + 1), self.d_features * self.d_seq_out)\n",
    "\t\telse:\n",
    "\t\t\t# Predict one output at a time\n",
    "\t\t\tself.fc = nn.Linear(d_hidden * (bidirectional + 1), self.d_features)\n",
    "\t\tself.autoregressive = autoregressive\n",
    "\n",
    "\tdef _shared_step(self, x, y):\n",
    "\t\t# x: (batch_size, d_seq_in, d_features)\n",
    "\t\t# y: (batch_size, d_seq_out, d_features)\n",
    "\t\tb_size = x.size(0)\n",
    "\n",
    "\t\tif not self.autoregressive:\n",
    "\t\t\t_, (h, _) = self.lstm(x)\n",
    "\t\t\th = h[-1, :, :]\n",
    "\t\t\ty_pred = self.fc(h).view(b_size, self.d_seq_out, self.d_features)\n",
    "\t\telse:\n",
    "\t\t\ty_pred = []\n",
    "\t\t\t_, (h, c) = self.lstm(x)\n",
    "\t\t\toutput = self.fc(h[-1, :, :]).unsqueeze(1)\n",
    "\t\t\ty_pred.append(output)\n",
    "\n",
    "\t\t\t# Autoregressive forecasting\n",
    "\t\t\tfor _ in range(self.d_seq_out - 1):\n",
    "\t\t\t\t_, (h, c) = self.lstm(output, (h, c))\n",
    "\t\t\t\toutput = self.fc(h[-1, :, :]).unsqueeze(1)\n",
    "\t\t\t\ty_pred.append(output)\n",
    "\n",
    "\t\t\ty_pred = torch.cat(y_pred, dim=1)\n",
    "\n",
    "\t\treturn y_pred, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9b6f3-b99a-4d31-afc6-1fbbdb0e5606",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68f7f3-a6c7-4021-9044-f575c8671e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c234ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor=\"val_loss/dataloader_idx_0\", patience=50)]\n",
    "\n",
    "use_logger = False\n",
    "if not use_logger:\n",
    "    logger = False\n",
    "else:\n",
    "    name = \"LSTM\"\n",
    "    callbacks.append(LearningRateMonitor())\n",
    "    logger = TensorBoardLogger(\n",
    "        \"logs/2-baseline\",  # change to notebook number\n",
    "        name=name, \n",
    "        default_hp_metric=False\n",
    "    )\n",
    "\n",
    "trainer_hparams = dict(\n",
    "    accelerator='auto', \n",
    "    devices=1,\n",
    "    max_epochs=500,\n",
    "    log_every_n_steps=10,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "trainer = pl.Trainer(**trainer_hparams)\n",
    "\n",
    "model = LSTM(d_hidden=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17074710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23e086de-015f-4fd9-bb1a-ac8eade6fa0e",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af29a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_forecast(sample, fcast, title=None, display=True):\n",
    "    \"\"\"Plots forecast of tank levels and settings for one sample.\n",
    "    Args:\n",
    "        sample: torch.Tensor, shape (seq_len, 3), sample from dataloader\n",
    "        fcast: torch.Tensor, shape (pred_len, 3), forecast of sample\n",
    "        title: str, title of plot\n",
    "        display: bool, if True, plot is displayed, else returned\n",
    "    \"\"\"\n",
    "    x1, x2 = sample\n",
    "    pred_x2 = np.squeeze(fcast)\n",
    "    x = np.concatenate((x1, x2))\n",
    "    colors = [\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#d62728',  # brick red\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#17becf',  # blue-teal\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#bcbd22',  # curry yellow-green\n",
    "        '#9467bd',  # muted purple\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "        '#7f7f7f',  # middle gray\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for sig, name, c in zip([x[:, 0], x[:, 1], x[:, 2]],\n",
    "                            ['h1', 'h2', 'h3'],\n",
    "                            colors[:3]):\n",
    "        fig.add_trace(go.Scatter(x=np.array(range(x.shape[0])), y=sig, name=name,\n",
    "                      mode=\"lines\", opacity=1, line=dict(color=c)))\n",
    "    for sig, name, c in zip([pred_x2[:, 0], pred_x2[:, 1], pred_x2[:, 2]],\n",
    "                            ['pred_h1', 'pred_h2', 'pred_h3'],\n",
    "                            colors[3:7]):\n",
    "        fig.add_trace(go.Scatter(x=np.array(range(x1.shape[0], x1.shape[0] + x2.shape[0])), y=sig, name=name,\n",
    "                      mode=\"lines\", opacity=1, line=dict(color=c, dash=\"dot\")))\n",
    "\n",
    "    fig.add_vline(x=len(x1), line_dash=\"dash\")\n",
    "    fig.update_xaxes(tick0=0, dtick=200)\n",
    "    fig.update_xaxes(title_text=r'time')\n",
    "    fig.update_layout(width=800, height=500,\n",
    "                      font_family=\"Serif\", font_size=14,\n",
    "                      margin_l=5, margin_t=50, margin_b=5, margin_r=5)\n",
    "    if title is not None:\n",
    "        fig.update_layout(title=title)\n",
    "    if display:\n",
    "        fig.show()\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "def fcast_overview(datamodule, model, idx=0, title=None, save_path=None):\n",
    "    \"\"\"Plots forecast of tank levels and settings.\n",
    "    All scenarios are plotted in two figures. Combines plot_sample_forecast() and fcast_overview_separate().\n",
    "    Args:\n",
    "        datamodule: DataModule\n",
    "        model: Model\n",
    "        idx: int, index of sample to plot\n",
    "        title: str, title of plot\n",
    "        save_path: str, path to save plot\n",
    "    \"\"\"\n",
    "    model = model.to(model.visualization_device)\n",
    "    model.eval()\n",
    "    datasets = datamodule.ds_dict\n",
    "\n",
    "    # plot water levels\n",
    "    n_rows = 3\n",
    "    n_cols = 3\n",
    "    fig = subplots.make_subplots(rows=n_rows, cols=n_cols, shared_xaxes=True, vertical_spacing=0.02)\n",
    "    for i, (scenario, ds) in enumerate(datasets.items()):\n",
    "        if i >= n_rows * n_cols:\n",
    "            break\n",
    "        sample = ds[idx]\n",
    "        x = torch.tensor(sample[0]).unsqueeze(0)\n",
    "        fcast = model(x).cpu().detach().numpy()\n",
    "        fcast_plot = plot_sample_forecast(sample, fcast, title=scenario, display=False)\n",
    "        for j in range(6):\n",
    "            fig.add_trace(fcast_plot.data[j], row=(i//n_cols)+1, col=(i%n_cols)+1)\n",
    "        fig.update_xaxes(tick0=0, dtick=50)\n",
    "        fig.update_yaxes(title_text=scenario + f\" [DataLoader {i}]\", row=(i//n_cols)+1, col=(i%n_cols)+1)\n",
    "\n",
    "    fig.add_vline(x=x.size(1), line_dash=\"dash\")\n",
    "    for col in range(n_cols):\n",
    "        fig.update_xaxes(title_text=r'time', row=n_rows, col=col+1)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_layout(title=f\"Water Level Predictions by {title}\")\n",
    "    # export figure\n",
    "    if save_path is not None:\n",
    "        # if the path does not exist, create it\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        fig.write_image(save_path + f\"water_levels_{title}.png\", width=1200, height=800)\n",
    "    else:\n",
    "        fig.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcast_overview(dm, model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d938c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('basic-pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e13bc41746ede79f3cfe704424fb4343b96315e8e9d484218a5a332a906daae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
